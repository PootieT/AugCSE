{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Running Evaluation - NL-Augmenter (All Operations)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLKCY5U981Y_"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook can be used to evaluate the different transformations and filters present in the NL-Augmenter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs7AFy_7ABgb"
      },
      "source": [
        "# Setting up the repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1_mQyFIjP2E"
      },
      "source": [
        "**Clone the repository!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiSoM2Zpf9OC"
      },
      "source": [
        "!git clone https://github.com/GEM-benchmark/NL-Augmenter.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpGIM8_YRjRn"
      },
      "source": [
        "%cd NL-Augmenter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfPYdyMDAQYm"
      },
      "source": [
        "## Installation of requirements\n",
        "\n",
        "By default, all the filters (both light and heavy) and the light transformations will be installed (through base requirements). To make sure all the heavy transformations are also installed, run the following commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V6arD-ojUy5",
        "outputId": "d8fbbd51-9c90-4c9e-e565-3c0e608503a2"
      },
      "source": [
        "TRANSFORMATIONS_DIR = 'transformations'\n",
        "\n",
        "import os\n",
        "\n",
        "# Install tesseract for OCR transformation\n",
        "!sudo apt install -y libleptonica-dev libtesseract-dev tesseract-ocr{,-eng,-osd}\n",
        "\n",
        "# Install base project requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# English Spacy model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Install requirements for every transformation (this is necessary to run the evaluation script)\n",
        "for transformation_dir in os.listdir(TRANSFORMATIONS_DIR):\n",
        "  transformation_path = os.path.join(TRANSFORMATIONS_DIR, transformation_dir)\n",
        "  if os.path.isdir(transformation_path) and 'requirements.txt' in os.listdir(transformation_path):\n",
        "    transformation_reqs = os.path.join(transformation_path, 'requirements.txt')\n",
        "    !pip install -r \"$transformation_reqs\"\n"
      ],
      "execution_count": null,
      "outputs": [
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2EBjYPyqCPa"
      },
      "source": [
        "**Note:** \n",
        "The requirements for some transformations and filters may have been disabled (during the merging of PR). If your transformation or filter has disabled requirements, please install them separately by uncommenting the below command and adding the relevant transformation or filter name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nypdK4y-qOYB"
      },
      "source": [
        "# ! pip install transformations_or_filters_folder/transformation_or_filter_name/requirements-disabled.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcOTesRpRwvl"
      },
      "source": [
        "# Transformations\n",
        "\n",
        "Each transformation may support multiple task types. Depending on the task types used in the transformation, the datasets and settings of the model to perform evaluation may differ. Please refer to the [evaluation](https://github.com/GEM-benchmark/NL-Augmenter/tree/main/evaluation) page for different settings. \n",
        "\n",
        "If you want to use any dataset other than the ones shown in the notebook for your task type, you can add that to the evaluation engine by following the instructions specified [here](https://github.com/GEM-benchmark/NL-Augmenter/tree/main/evaluation#evaluation-guideline-and-scripts)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ektH5sRF575"
      },
      "source": [
        "## Sentence Operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzd_bm8gk844"
      },
      "source": [
        "If your transformation is a SentenceOperation one, evaluate it by runnning the 4 cells below and paste the numbers in the excel sheet. (Note that you only need to change the model name: each of the below setting represents 4 different settings of models and datasets - to confirm what you are testing, you can check the [evaluation](https://github.com/GEM-benchmark/NL-Augmenter/tree/main/evaluation) page.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx_vsFTsk8ci",
        "outputId": "13964554-4758-474e-bd56-8734fb0f83d3"
      },
      "source": [
        "!python evaluate.py -t GeoNamesTransformation -task \"TEXT_CLASSIFICATION\" -m \"textattack/roberta-base-imdb\" -d \"imdb\" -p 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "Downloading: 100% 908/908 [00:00<00:00, 679kB/s]\n",
            "Downloading: 100% 1.63G/1.63G [00:45<00:00, 35.6MB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 829kB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 626kB/s]\n",
            "Downloading: 100% 1.36M/1.36M [00:01<00:00, 1.06MB/s]\n",
            "Downloading: 100% 26.0/26.0 [00:00<00:00, 20.0kB/s]\n",
            "Downloading: 100% 665/665 [00:00<00:00, 441kB/s]\n",
            "Downloading: 100% 548M/548M [00:15<00:00, 36.0MB/s]\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 1.15MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 631kB/s]\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 1.48MB/s]\n",
            "Loading <imdb> dataset to evaluate <textattack/roberta-base-imdb> model.\n",
            "Downloading: 100% 559/559 [00:00<00:00, 376kB/s]\n",
            "Downloading: 100% 501M/501M [00:23<00:00, 21.5MB/s]\n",
            "Some weights of the model checkpoint at textattack/roberta-base-imdb were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading: 100% 798k/798k [00:00<00:00, 874kB/s] \n",
            "Downloading: 100% 456k/456k [00:00<00:00, 627kB/s]\n",
            "Downloading: 100% 239/239 [00:00<00:00, 171kB/s]\n",
            "Downloading: 100% 25.0/25.0 [00:00<00:00, 18.7kB/s]\n",
            "Downloading: 4.61kB [00:00, 3.70MB/s]       \n",
            "Downloading: 2.02kB [00:00, 2.29MB/s]       \n",
            "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown size, total: 207.28 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b...\n",
            "Downloading: 100% 84.1M/84.1M [00:07<00:00, 11.8MB/s]\n",
            "Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b. Subsequent calls will reuse this data.\n",
            "Here is the performance of the model textattack/roberta-base-imdb on the test[:20%] split of the imdb dataset\n",
            "The accuracy on this subset which has 1000 examples = 95.0\n",
            "Applying transformation:\n",
            "100% 1000/1000 [00:02<00:00, 353.73it/s]\n",
            "Finished transformation! 20646 examples generated from 1000 original examples, with 20646 successfully transformed and 0 unchanged (1.0 perturb rate)\n",
            "Here is the performance of the model on the transformed set\n",
            "The accuracy on this subset which has 20646 examples = 94.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-wLBvkbsfuj"
      },
      "source": [
        "!python evaluate.py -t GeoNamesTransformation -task \"TEXT_CLASSIFICATION\" -m \"textattack/roberta-base-SST-2\" -d \"sst2\" -p 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP4Wyxy4nCOA"
      },
      "source": [
        "!python evaluate.py -t GeoNamesTransformation -task \"TEXT_CLASSIFICATION\" -m \"textattack/bert-base-uncased-QQP\" -d \"qqp\" -p 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfCVqpa8nvLg"
      },
      "source": [
        "!python evaluate.py -t GeoNamesTransformation -task \"TEXT_CLASSIFICATION\" -m \"roberta-large-mnli\" -d \"multi_nli\" -p 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQK8ceuKF9W6"
      },
      "source": [
        "## QuestionAnswer Operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFBsO3-7GyTp"
      },
      "source": [
        "If your transformation is a question answering one, run the below command with your transformation name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnhN8DnZGBcn",
        "outputId": "479ba1ba-4fbf-463f-a0d1-863ea0d82ce7"
      },
      "source": [
        "!python evaluate.py -t QuestionInCaps -task \"QUESTION_ANSWERING\" -m \"mrm8488/bert-tiny-finetuned-squadv2\" -d \"squad\" -p 20"
      ],
      "execution_count": null,
      "outputs": [
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU4MeoylG-d6"
      },
      "source": [
        "## Tagging Operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL8zDagJSF87"
      },
      "source": [
        "If your transformation uses tagging operation, run the below command with your transformation name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPMYJOUnG_l4",
        "outputId": "7de2f9ea-98c5-43f1-e132-b81acce33924"
      },
      "source": [
        "!python evaluate.py -t LongerLocationNer -task \"TEXT_TAGGING\" -m \"dslim/bert-base-NER\" -p 20"
      ],
      "execution_count": null,
      "outputs": [
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRza51IYSWKm"
      },
      "source": [
        "# Filters\n",
        "\n",
        "Each filter may support multiple task types. Depending on the task types used in the filter, the datasets and settings of the model to perform evaluation may differ. Please refer to the [evaluation](https://github.com/GEM-benchmark/NL-Augmenter/tree/main/evaluation) page for different settings.\n",
        "\n",
        "If you want to use any dataset other than the ones shown in the notebook for your task type, you can add that to the evaluation engine by following the instructions specified [here](https://github.com/GEM-benchmark/NL-Augmenter/tree/main/evaluation#evaluation-guideline-and-scripts)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE3Ax1e1SZmW"
      },
      "source": [
        "## Sentence Operation\n",
        "\n",
        "If your filter is a SentenceOperation one, evaluate it by runnning the 4 cells below and paste the numbers in the excel sheet. (Note that you only need to change the model name: each of the below setting represents 4 different settings of models and datasets - to confirm what you are testing, you can check the evaluation page.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiGjtVb_YBck",
        "outputId": "15b85236-fafd-45a3-d05e-cc7352c59e9b"
      },
      "source": [
        "!python evaluate.py -f TextContainsNumberFilter -task \"TEXT_CLASSIFICATION\" -m \"textattack/roberta-base-imdb\" -d \"imdb\" -p 20"
      ],
      "execution_count": null,
      "outputs": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYSJGKIjYG1y"
      },
      "source": [
        "!python evaluate.py -f TextContainsNumberFilter -task \"TEXT_CLASSIFICATION\" -m \"textattack/roberta-base-SST-2\" -d \"sst2\" -p 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2cebsGUYGij"
      },
      "source": [
        "!python evaluate.py -f TextContainsNumberFilter -task \"TEXT_CLASSIFICATION\" -m \"textattack/bert-base-uncased-QQP\" -d \"qqp\" -p 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipf8zdD1UaKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc50220-4fca-40e0-fd15-334fc8364a6d"
      },
      "source": [
        "!python evaluate.py -f TextContainsNumberFilter -task \"TEXT_CLASSIFICATION\" -m \"roberta-large-mnli\" -d \"multi_nli\" -p 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'evaluate.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JMfzC1WYnhb"
      },
      "source": [
        "## QuestionAnswer Operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlDqdSEhYy_0"
      },
      "source": [
        "If your filter is a question answering one, run the below command with your filter name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zusFVRM9Y0LF",
        "outputId": "66506619-8e21-445f-adfa-b767912a73f1"
      },
      "source": [
        "!python evaluate.py -f  NumericQuestion -task \"QUESTION_ANSWERING\" -m \"mrm8488/bert-tiny-finetuned-squadv2\" -d \"squad\" -p 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"evaluate.py\", line 56, in <module>\n",
            "    implementation = get_implementation(args.filter, \"filters\")\n",
            "  File \"/content/NL-Augmenter/TestRunner.py\", line 235, in get_implementation\n",
            "    for operation in OperationRuns.get_all_operations(search):\n",
            "  File \"/content/NL-Augmenter/TestRunner.py\", line 215, in get_all_operations\n",
            "    t_py = import_module(f\"{search}.{folder}\")\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"/content/NL-Augmenter/filters/code_mixing/__init__.py\", line 1, in <module>\n",
            "    from .filter import *\n",
            "  File \"/content/NL-Augmenter/filters/code_mixing/filter.py\", line 2, in <module>\n",
            "    from ftlid import identify_language\n",
            "ModuleNotFoundError: No module named 'ftlid'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OukwSxmFZyGc"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "If there are any issues or error while running the notebook, please feel free to raise an issue [here](https://github.com/GEM-benchmark/NL-Augmenter/issues)."
      ]
    }
  ]
}